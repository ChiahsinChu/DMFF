# DiffTraj
## 1. Theory
DiffTraj provides a support to do gradient calculation on a loss function from trajectory. First a NVE (leap-frog Verlet) simulation is conducted, and then compute the gradient from the trajectory.
### 1.1 NVE simulation
NVE simulation follows the leap-frog Verlet integration method, just like in openMM. The positions and velocities stored in the context are offset from each other by half a time step. In each step, they are updated as follows:

```math
\begin{aligned}
\mathbf{v}_i(t+\Delta t / 2) & =\mathbf{v}_i(t-\Delta t / 2)+\mathbf{f}_i(t) \Delta t / m_i \\
\mathbf{r}_i(t+\Delta t) & =\mathbf{r}_i(t)+\mathbf{v}_i(t+\Delta t / 2) \Delta t
\end{aligned}
```
where $\mathbf{v}_i$ is the velocity of particle i, $\mathbf{r}_i$ is its position, $\mathbf{f}_i$ is the force acting on it which is got from auto-differential calculation on energy, $m_i$ is its mass, and $\Delta t$ is the time step.
### 1.2 Gradient calculation
Naive using auto-differential `jax.grad` of jax on the trajectory may cause the OOM problem, here in DiffTraj, we use the adjoint method to run a reverse calculation utilizing the time reversibility of NVE integrator to accumulate the gradient. The Loss function is,

```math
L\left( \mathbf{z}\left( t_1 \right) \right) =L\left( \mathbf{z}\left( t_0 \right) +\int_{t_0}^{t_1}{f\left( \mathbf{z}\left( t \right) ,\ t,\ \theta \right) dt} \right) =L\left( \text{ODESolve}\left( \mathbf{z}\left( t_0 \right) ,\ f,\ t_0,\ t_1,\ \theta \right) \right) 
```

where $L$ is the loss function, $\mathbf{z}\left( t_1 \right)$ is the state(velocity and position) at time $t_1$, $f$ is the integrator, and $\theta$ is the parameter. The gradient calculation starts at final state, the adjoint state is defined as $\mathbf{a}\left( t \right) =\frac{\partial L}{\partial \mathbf{z}\left( t \right)}$, the gradient is calculated using chain rule, 

```math
\frac{\partial L}{\partial \mathbf{z}\left( t_0 \right)}=\frac{\partial L}{\partial \mathbf{z}\left( t_1 \right)}\frac{\partial \mathbf{z}\left( t_1 \right)}{\partial \mathbf{z}\left( t_0 \right)}
```

```math
\frac{\partial L}{\partial \theta}=\mathbf{a}\left( t_1 \right) ^T\frac{\partial f\left( \mathbf{z}\left( t_0 \right) ,\ \theta \right)}{\partial \theta}
```

When the Loss function is a function of trajectory, the calculation follows:

[![grad.png](https://i.postimg.cc/pdm6fWX1/grad.png)](https://postimg.cc/nst2Ztmv)

### References
1. [Neural Ordinary Differential Equations](https://doi.org/10.48550/arXiv.1806.07366)

## 2. Function module

Class `Loss_Generator`:
- Set the condition of simulation.
- Contains the leap-frog Verlet integration method.

Function `ode_fwd`:
- Run the NVE simulation.
- Get the trajectory.

Function `generate_Loss`:
- Generate the Loss function.

## 3. How to use it
Here we would tell you how to use Loss_Generator and get gradient, we also give an [example](../../examples/msd_opt/fit-msd.ipynb) introducing how to use this module.
- Initialization: Create an instance of the `Loss_Generator`.

```python
Generator = Loss_Generator(f_nout, box, init_state['pos'][0], mass, dt, nsteps, nout, cov_map, rc, efunc)
```
You can use the Generator to only do a NVE simulation or do both NVE simulation and gradient calculation.

- Only do a NVE simulation
```python
final_state, traj = Generator.ode_fwd(initial_state, params)
```
- Define Loss function and get gradient
```python
Loss = Generator.generate_Loss(L, has_aux=True, metadata=metadata)
v, g = value_and_grad(Loss, argnums=(1))(init_state, params)
```
